<a name="br1"></a> 

**AeroNex**

**University of Washington, GIX - 513 Course**

**AeroNex: Aerial Intelligence**

**for Precision Operations**

**Ankit Shaw\***

University of Washington

ankit25@uw.edu

**Abstract**

In the era of advancing technology, the AeroNex project

pioneers a novel approach to environmental hazard detec-

tion and inventory management through the integration of

cutting-edge drone technology and image processing tech-

niques. Employing a drone as the main data acquisition

tool, AeroNex captures image and video data in hazardous

environments, particularly focusing on construction sites.

These images undergo a comprehensive image process-

ing for distinct feature extraction such as edge detection.

Subsequently, we use Machine Learning algorithms to pro-

cessed the images for object classiﬁcation, enabling precise

inventory management through object detection and count-

ing. AeroNex represents a holistic solution for monitoring

and managing hazardous environments (ﬁg.3), pushing the

boundaries of technology for enhanced safety and efﬁciency.

**Xinyi Zhou\***

University of Washington

xinyigix@uw.edu

**Ravinder Gulla\***

University of Washington

rgulla@uw.edu

**Keywords**

Drone, Image Processing, Sobel Filter, Laplacian Filter,

Black and White Filter, Colorization Filter, Morphological Fil-

ters, YOLOv3, Object Classiﬁcation, Inventory Management,

Hazardous Environments, Edge Detection.

Figure1: AeroNex website



<a name="br2"></a> 

**AeroNex**

**University of Washington, GIX - 513 Course**

Figure3: Hazardous environments

Figure3: Use case



<a name="br3"></a> 

**AeroNex**

**University of Washington, GIX - 513 Course**

Technology Integration (ﬁg.4):

b. Collision Avoidance: Sensors, such as obstacle detection

camera, is employed to ensure collision avoidance. The

drone dynamically adjusts its ﬂight path based instructions

provide in the code and navigates safely.

c. Path Following Algorithm: A path-following algorithm

guides the drone along the predeﬁned waypoints. This algo-

rithm takes into account the drone’s current position, desired

trajectory, and obstacle avoidance measures.

a. Drone Technology: We used a drone as a central compo-

nent of the project for data collection in hazardous environ-

ments, due to their agility, accessibility, and versatility.

b. Image Processing Techniques: We employed advanced

image processing techniques, such as Sobel and Laplacian

ﬁlters, colorization, and morphological ﬁlters for extracting

meaningful features from the captured images.

d. Altitude Control: We maintain a 10 ft altitude to ensure

that the drone maintains a consistent height above ground

level. This is crucial for capturing images at the desired res-

olution and perspective.

c.YOLOv3 Model: We use YOLOv3 model for object classi-

ﬁcation and inventory management for its real-time capabil-

ities and accuracy in detecting and counting objects in the

images. (ﬁg.3)

e. Energy Management: The navigation system also consid-

ers energy management to optimize the drone’s ﬂight time.

Efﬁcient route planning and dynamic adjustments contribute

to extending the drone’s operational duration. The drone’s

ﬂying time is 12 minutes with 3 batteries. The navigation

time for the drone to complete one ﬂight is programmed to

be under 2 minutes for all our simulations.

**Navigation**

A drone is employed for seamless navigation through haz-

ardous environments, capturing images from vantage points

inaccessible to humans. The autonomous ﬂight capabilities

of the drone ensure efﬁcient coverage of the designated

areas, facilitating comprehensive data collection. The nav-

igation aspect of the AeroNex project is a crucial element

that involves guiding the drone through inaccessible envi-

ronments to capture images. The autonomous ﬂight capabil-

ities of the drone play a pivotal role in ensuring the efﬁcient

coverage of designated areas. AeroNex involves meticulous

planning, autonomous ﬂight control, obstacle avoidance,

and integration with image capture mechanisms. This com-

prehensive approach ensures that the drone effectively nav-

igates hazardous environments, captures relevant data, and

contributes to the success of the project’s objectives.

Autonomous Drone Navigation using Tello Boost (ﬁg.5):

a. Waypoint Planning: Before deployment, the AeroNex sys-

tem plans a ﬂight path using predeﬁned waypoints. These

waypoints are strategically chosen to cover the entire haz-

ardous environment while ensuring optimal image capture.

**Integration with Image Capture**

The navigation process is tightly integrated with the image

capture mechanism. As the drone navigates through the

hazardous environment, it captures images at predeﬁned

intervals and locations. The coordinated efforts of navigation

and image capture contribute to the comprehensive data

acquisition needed for subsequent analyses.

**Safety Protocols:** To ensure the safety of both the drone

and the environment, the navigation system adheres to strict

safety protocols. The program restricts the drone’s ﬂight

within predeﬁned boundaries, emergency landing proce-

dures.

Figure4: Technology integration

Figure5: Autonomous Drone Nav-

igation



<a name="br4"></a> 

**AeroNex**

**University of Washington, GIX - 513 Course**

**Filters**

Sobel Filter (ﬁg.7): Applied for edge detection, the Sobel ﬁl-

ter enhances the visibility of gradient changes in the images,

highlighting signiﬁcant features. The Sobel ﬁlter is a com-

monly used operator for edge detection in image process-

ing. It operates by convolving the image with a convolution

matrix, typically either Sobel-X or Sobel-Y, to highlight edges

along the horizontal or vertical axis. Sobel ﬁlter convolution

happens step by step

The heart of AeroNex’s image processing lies in the ap-

plication of advanced ﬁlters to the captured images. Tello

drone has a basic camera and we use Image ﬁlters to the

drone-captured images for diverse purposes, including

enhancing visual appeal and ensuring speciﬁc feature ex-

traction. They help correct distortions, enhance color, adapt

to environmental conditions, and contribute to a consistent

and standardized dataset for analysis. Filters are employed

for feature extraction of the images.

Convolution Process (ﬁg.6)

a. Place the Filter matrix (Feature Detector) over a pixel in

the image: The central pixel of the matrix aligns with the pix-

el in the image to which the convolution is being applied.

b. Multiply each element of the matrix by the corresponding

pixel value in the image: Multiply each value in the matrix by

the corresponding pixel value in the image covered by that

element of the matrix.

Figure7: Sobel ﬁlter

c. Sum the results: Sum up all the products obtained in step

2\.

d. Move the matrix to the next pixel and repeat: Slide the

matrix to the next pixel in the image and repeat the multipli-

cation and summation process.

e. Repeat for the entire image: Continue this process for ev-

ery pixel in the image, performing the convolution operation

at each location.

Sobel-X Convolution: The Sobel-X convolution is designed

to emphasize vertical edges in an image. The convolution

matrix for Sobel-X is:

| 1 | 0 | -1 |

Figure6: Convolution process

| 2 | 0 | -2 |

| 1 | 0 | 1 |

f. Generate a new image (convolved image): The result of

the convolution operation is a new image, often called the

“convolved image” or “gradient image.” This image highlights

areas where there are signiﬁcant intensity changes along

the vertical axis, indicating potential vertical edges.

Sobel-Y Convolution: The Sobel-Y convolution is similar but

emphasizes horizontal edges. The Sobel-Y matrix is:

| 1 | 2 | 1 |

| 0 | 0 | 0 |

| -1 | -2 | -1 |



<a name="br5"></a> 

**AeroNex**

**University of Washington, GIX - 513 Course**

The steps for Sobel-Y convolution are the same as for So-

bel-X, but the emphasis is on capturing intensity changes

along the horizontal axis.

| 1 | 1 | 1 |

| 1 | -8 | 1 |

| 1 | 1 | 1 |

In summary, Sobel ﬁlter convolution involves moving a small

matrix (Sobel-X or Sobel-Y) over each pixel in an image,

multiplying the matrix values by the corresponding pixel

values, summing the results, and producing a new image

that highlights edges along the speciﬁed axis. This process

enhances the visibility of edges and is particularly useful for

feature extraction in image analysis tasks. We use Sobel-X

ﬁlter.

In summary, the Laplacian convolution matrix focuses on de-

tecting changes in intensity in the immediate neighborhood

of each pixel, making it a powerful tool for edge detection

and feature enhancement in image processing.

Figure9: Black and white ﬁlter

Black and White Filter (ﬁg.9): Simplifying the images to

grayscale aids in focusing on structural elements, providing

a foundation for subsequent analyses.

Figure8: Laplacian ﬁlter

Colorization Filter (ﬁg.10): This ﬁlter enhances visual infor-

mation by introducing color, providing a clearer representa-

tion of the environment and aiding in object recognition.

Morphological Filters (ﬁg.11): Employed for shape analy-

sis and manipulation, morphological ﬁlters play a crucial role

in reﬁning the extracted features and preparing the images

for further processing.Morphological operations, speciﬁcally

dilation and erosion, are fundamental techniques in image

processing that modify the shape and structure of objects

within an image. These operations involve the use of a

structuring element (also known as a kernel) to process the

Laplacian Filter (ﬁg.8): Designed for detecting edges and

ﬁne details, the Laplacian ﬁlter contributes to a more reﬁned

analysis of the images, capturing intricate structures. The

Laplacian ﬁlter is an image processing operator used for

edge detection and the enhancement of ﬁne details. It high-

lights regions of rapid intensity change in an image, making

it particularly useful for identifying edges. The convolution

matrix for the Laplacian ﬁlter is:

Figure10: Colorization ﬁlter



<a name="br6"></a> 

**AeroNex**

**University of Washington, GIX - 513 Course**

pixel values in an image. Let’s delve into the detailed expla-

nations of the convolution process for both dilation and ero-

sion.

Detailed explanations of the convolution matrices associated

with each ﬁlter contribute to a comprehensive understanding

of the image processing techniques employed in AeroNex.

**Dilation Convolution:** Dilation is a morphological operation

that enhances the brighter regions in an image, making ob-

jects more prominent. The convolution process involves the

following steps:

\*Structuring Element (Kernel):\*

| 0 | 1 | 0 |

| 1 | 1 | 1 |

| 0 | 1 | 0 |

The result is a new image, often called the “dilated image,”

where brighter regions have expanded, and objects are

made more prominent.

**Erosion Convolution:** Erosion is another morphological

operation that shrinks the brighter regions in an image. The

convolution process involves the following steps:

\*Structuring Element (Kernel):\*

Figure11: Morphological ﬁlter

Figure12: Process of image classiﬁcation

Classiﬁcation (ﬁg.12)

YOLOv3 (You Only Look Once version 3): Image Classi**-**

ﬁcation for AeroNex

| 1 | 1 | 1 |

| 1 | 1 | 1 |

| 1 | 1 | 1 |

YOLOv3 is a state-of-the-art real-time object detection mod-

el that excels in identifying and classifying multiple objects

within an image. Its architecture allows for efﬁcient and ac-

curate object recognition, making it a suitable choice for the

AeroNex project’s inventory management component.

**Architecture:** YOLOv3 adopts a deep neural network ar-

chitecture based on convolutional neural networks (CNNs).

It divides the input image into a grid and predicts bounding

boxes and class probabilities for each grid cell.

The result is a new image, often called the “eroded image,”

where brighter regions have contracted, and ﬁner details are

retained.

In summary, the convolution of morphological ﬁlters, such as

dilation and erosion, involves sliding a structuring element

over each pixel in the image, modifying pixel values based

on the speciﬁc rules outlined for each operation. Dilation ex-

pands brighter regions, while erosion contracts them. These

morphological operations are essential for tasks such as

image segmentation, noise reduction, and feature extraction

in image processing applications.

**Bounding Box Prediction:** YOLOv3 predicts bounding

boxes by regressing the coordinates (x, y, width, height) for



<a name="br7"></a> 

**AeroNex**

**University of Washington, GIX - 513 Course**

**Bounding Box Prediction:** YOLOv3 predicts bounding

boxes by regressing the coordinates (x, y, width, height) for

each object within a grid cell. Multiple bounding boxes are

predicted for each grid cell, each associated with a conﬁ-

dence score representing the model’s conﬁdence in the box

containing an object.

on the images ofﬂine, and helps in the crucial and prompt

decision-making and inventory management.

**Class Prediction:** YOLOv3 predicts class probabilities for

each bounding box. The model performs multi-label clas-

siﬁcation, allowing for the identiﬁcation of multiple objects

within a single bounding box. The class predictions are con-

ditioned on the presence of an object within a bounding box,

contributing to the overall detection and classiﬁcation accu-

racy.

Feature Pyramid Network (FPN): YOLOv3 incorporates

FPN, which enhances the network’s ability to detect objects

at different scales. It combines features from different levels

of the network, allowing the model to capture both ﬁne and

coarse details.

**Anchor Boxes:** YOLOv3 employs anchor boxes to improve

bounding box predictions. These anchor boxes are pre-

deﬁned shapes that the model uses to reﬁne its predictions,

leading to more accurate localization.

Figure13: Aero image classiﬁer

\*Non-Maximum Suppression (NMS):\*

To eliminate duplicate detections, YOLOv3 employs NMS.

This post-processing step selects the most conﬁdent bound-

ing box among those with overlapping predictions and sup-

presses redundant boxes.

**Inventory Management:**

YOLOv3 plays a pivotal role in inventory management by

accurately identifying and classifying objects in the images.

The model’s ability to handle multiple objects in a single im-

age contributes to the efﬁciency of counting and managing

inventory in hazardous environments.

**Integration with Navigation and Image Processing:**

YOLOv3 seamlessly integrates with the overall AeroNex

system, complementing the navigation system’s autono-

Classiﬁcation for AeroNex Images (ﬁg.13)

**Input Processing:** Images captured by the drone that are

sent to the ﬁlters are passed into the YOLOv3 model as in-

put for object detection and classiﬁcation. YOLOv3 operates



<a name="br8"></a> 

**AeroNex**

**University of Washington, GIX - 513 Course**

mous ﬂight capabilities and the image processing pipeline’s

feature extraction. This integration creates a comprehensive

solution for hazard detection and inventory management.

**Optimization:** YOLOv3 may undergo optimization tech-

niques, such as model quantization or pruning, to reduce its

computational requirements, enabling efﬁcient deployment

on edge devices like the drone.

Performance Evaluation (ﬁg.14)

**Conclusion:** YOLOv3’s robust object detection capabili-

ties signiﬁcantly contribute to the success of the AeroNex

project. Its real-time processing, accuracy in object classiﬁ-

cation, and seamless integration make it a powerful tool for

hazard detection and inventory management in challenging

environments.

**Metrics:** The performance of YOLOv3 in the AeroNex proj-

ect is evaluated using metrics such as precision, recall, and

F1 score. These metrics provide insights into the model’s

accuracy in detecting and classifying objects in hazardous

environments. It can be seen from the image that the orig-

inal image and the image after applying the b&W ﬁlter are

being classiﬁed differently by the YOLO Classiﬁer.

**Future Research**

**Multi threaded process:** Currently the drone runs on a

single process. For this reason, the drone operations hap-

pen synchronously i.e., the drone ﬂies along the path, stops,

takes images/video for 2 seconds, then moves the next way-

point and then takes images/video for 2 seconds. We want

to increase

**SLAM:** This effective technology will improve the localiza-

tion and mapping criteria for the drone. As the drone moves

through the environment, it updates its estimate of its own

location while also updating the map based on sensor data.

SLAM algorithms handle the uncertainty associated with

both localization and mapping, providing a more robust solu-

tion.

Figure14: 13: Aero image classiﬁ-

er

**Real-time Adjustments:** The navigation system continuous-

ly monitors the drone’s surroundings and makes real-time

adjustments to the ﬂight path using on Board Collision De-

tection. This adaptability ensures that unexpected obstacles

or environmental changes do not disrupt the mission.

**Return-to-Base Functionality:** In the case of low battery

levels, completion of the mission, or emergency situations,

the drone will be programmed with a return-to-base function-

ality.

Figure14: Performance evaluation



<a name="br9"></a> 

**AeroNex**

**University of Washington, GIX - 513 Course**

**References**

Code and Readme ﬁle : AeroNex\_Code.pdf

Introduction to Convolutional Neural Netrwork: by Jianxin Xu

2017\. http://cs.nju.edu.cn/wujx/paper/CNN.pdf

Gradient-Based Learning applied to Document Recognition:

Yan LeCun, Leon bottou, Yoshua bengio and Patrick Haff-

ner. https://www.iro.umontreal.ca/~lisa/pointeurs/lecun-01a.

pdf

https://medium.com/@er\_95882/convolution-image-ﬁl-

ters-cnns-and-examples-in-python-pytorch-bd3f3ac5df9c

https://www.v7labs.com/blog/neural-networks-activa-

tion-functions#:~:text=ReLU%20activation%20function%20

should%20only,depth%20greater%20than%2040%20layers

https://www.v7labs.com/blog/neural-networks-activa-

tion-functions#:~:text=ReLU%20activation%20function%20

should%20only,depth%20greater%20than%2040%20layers

https://dl.acm.org/doi/abs/10.1145/3459104.3459114?ca-

sa\_token=28r7Fb7V6QcAAAAA:pbVCAopHHhafvktoMgol-

NX4SePRTZC217IGbkL9JXVbrasQWaFCnqstFCoFTowz-

V5QjAU-k\_nrTd

https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21495?-

casa\_token=vW8OGavbA4MAAAAA:b4lcd7w4PoHVqPvP-

pfH\_Srlt9Z\_m4hiTyndmx6sYKw-c1f\_PtUhO\_xw\_UVhvacqx-

dTk9tpoagMfpAvk

https://ieeexplore.ieee.org/abstract/document/5513152?ca-

sa\_token=5GgRXV\_49UMAAAAA:zo4QrVJId3KJx1a-gg-

HvpV5Gd3xi9NrPh4\_2ogmKHfuymss8TUsPQQ1tZiQOzl-

NcFO\_yu6-B


